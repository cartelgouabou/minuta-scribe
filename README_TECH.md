# Minuta - Documentation Technique

## ğŸ“‹ Table des matiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture](#architecture)
3. [Stack technique](#stack-technique)
4. [Structure du projet](#structure-du-projet)
5. [Flux de donnÃ©es](#flux-de-donnÃ©es)
6. [SchÃ©mas](#schÃ©mas)
7. [Installation et dÃ©veloppement](#installation-et-dÃ©veloppement)
8. [Configuration](#configuration)
9. [API Documentation](#api-documentation)
10. [Tests](#tests)
11. [DÃ©ploiement](#dÃ©ploiement)
12. [Contributions](#contributions)

---

## Vue d'ensemble

**Minuta** est une application web **offline-first** pour la transcription en temps rÃ©el de rÃ©unions et la gÃ©nÃ©ration automatique de comptes rendus via un LLM. L'application permet Ã  l'utilisateur d'Ã©diter/annoter la transcription avant la gÃ©nÃ©ration du compte rendu.

### CaractÃ©ristiques principales

- **Offline-first** : Tout fonctionne localement. La transcription utilise Whisper local et la gÃ©nÃ©ration de compte rendu utilise Ollama avec des modÃ¨les LLM locaux (Mistral 7B et Llama 3.2 3B). Choix du modÃ¨le disponible dans l'interface (v2.0)
- **Temps rÃ©el** : Transcription partielle toutes les 15 secondes pendant l'enregistrement
- **Multi-langues** : Support franÃ§ais et anglais
- **GPU automatique** : DÃ©tection et utilisation automatique du GPU si disponible (CUDA, MPS)
- **ThÃ¨me adaptatif** : Support dark/light mode avec toggle manuel
- **Ã‰dition du compte rendu** : PossibilitÃ© d'Ã©diter le compte rendu gÃ©nÃ©rÃ© avant export (v2.0)


## ğŸ“¦ Version 2.0 - Janvier 2026

### ğŸ‰ Nouvelles fonctionnalitÃ©s et amÃ©liorations

**Version 2.0** apporte des amÃ©liorations significatives pour une meilleure expÃ©rience utilisateur et une installation simplifiÃ©e.

#### âœ¨ FonctionnalitÃ©s ajoutÃ©es

1. **Ã‰dition du compte rendu**
   - Interface d'Ã©dition intÃ©grÃ©e avec `textarea` Ã©ditable
   - Indicateur visuel de modification (`isEdited` state)
   - Les exports PDF et TXT utilisent automatiquement le texte Ã©ditÃ©
   - Gestion d'Ã©tat sÃ©parÃ©e : `summary` (original) et `editedSummary` (modifiable)

2. **Choix entre modÃ¨les LLM**
   - Support de deux modÃ¨les : Mistral 7B Instruct et Llama 3.2 3B Instruct
   - SÃ©lection du modÃ¨le via dropdown dans l'interface
   - TÃ©lÃ©chargement automatique des deux modÃ¨les au dÃ©marrage via `start.sh`
   - Validation cÃ´tÃ© backend des modÃ¨les disponibles

3. **Scripts d'installation et dÃ©sinstallation amÃ©liorÃ©s**
   - **`start.sh`** :
     - DÃ©tection automatique du systÃ¨me d'exploitation (macOS, Linux, Windows)
     - Installation automatique de Docker (macOS, Ubuntu/Debian)
     - Installation automatique de Git Bash sur Windows si nÃ©cessaire
     - TÃ©lÃ©chargement automatique des modÃ¨les LLM (Mistral 7B et Llama 3.2 3B)
     - Messages d'aide spÃ©cifiques par plateforme
     - VÃ©rification de santÃ© des services Docker
   
   - **`uninstall.sh`** :
     - DÃ©sinstallation complÃ¨te de l'application
     - Suppression des conteneurs, images, volumes et rÃ©seaux Docker
     - Option de suppression de l'image Ollama
     - Confirmation avant suppression
     - DÃ©tection automatique du systÃ¨me d'exploitation

#### ğŸ”§ AmÃ©liorations techniques

- **Support multi-plateforme** :
  - DÃ©tection Windows via `OSTYPE` et `MSYSTEM`
  - Support Git Bash exclusif sur Windows (plus de WSL/PowerShell)
  - Messages d'erreur adaptÃ©s par plateforme

- **Configuration Nginx** :
  - Timeouts WebSocket augmentÃ©s (`proxy_read_timeout`, `proxy_send_timeout`)
  - Configuration optimisÃ©e pour les connexions longues

- **Gestion d'erreurs WebSocket** :
  - Timeout de 3 secondes avant affichage d'erreur
  - VÃ©rification de l'Ã©tat de connexion avant affichage
  - Gestion des fermetures normales vs erreurs

- **Variables CSS** :
  - Ajout de `--accent-color` et `--accent-rgb` pour cohÃ©rence visuelle
  - Styles pour textarea Ã©ditable avec focus states

#### ğŸ“ Changements dans le code

**Frontend** :
- `SummaryGenerator.tsx` : Ajout de l'Ã©tat `editedSummary` et `isEdited`
- `SummaryGenerator.tsx` : Remplacement de `div` par `textarea` Ã©ditable
- `SummaryActions.tsx` : Utilisation de `editedSummary` au lieu de `summary`
- `AudioRecorder.tsx` : AmÃ©lioration de la gestion d'erreurs WebSocket
- `Meeting.css` : Nouveaux styles pour textarea Ã©ditable et indicateur de modification
- `index.css` : Ajout des variables CSS d'accent

**Backend** :
- `routes/summary.py` : Validation des modÃ¨les LLM disponibles
- `services/ollama_service.py` : Support de deux modÃ¨les (Mistral et Llama)

**Infrastructure** :
- `docker/nginx.conf` : Configuration WebSocket amÃ©liorÃ©e
- `start.sh` : Logique d'installation multi-plateforme
- `uninstall.sh` : Script de dÃ©sinstallation complet

---

---

## Architecture

### Architecture gÃ©nÃ©rale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLIENT (Browser)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              React Frontend (Vite)                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚   Meeting    â”‚  â”‚   Prompts    â”‚  â”‚  Theme      â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   Component  â”‚  â”‚   Component  â”‚  â”‚  Context    â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚         â”‚                  â”‚                         â”‚  â”‚
â”‚  â”‚         â”‚ WebSocket        â”‚ REST API                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                  â”‚
             â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVER (FastAPI)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              FastAPI Application                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  WebSocket   â”‚  â”‚   REST API   â”‚  â”‚  Services  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   Handler    â”‚  â”‚   Routes     â”‚  â”‚            â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚         â”‚                  â”‚              â”‚          â”‚  â”‚
â”‚  â”‚         â”‚                  â”‚              â”‚          â”‚  â”‚
â”‚  â”‚         â–¼                  â–¼              â–¼          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚   Whisper    â”‚  â”‚   Ollama     â”‚  â”‚  SQLite     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   Service    â”‚  â”‚   Service    â”‚  â”‚  Database   â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ollama (Docker)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Ollama Server                            â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚  â”‚
â”‚  â”‚  â”‚   Mistral    â”‚  â”‚    Llama     â”‚                   â”‚  â”‚
â”‚  â”‚  â”‚   7B Inst    â”‚  â”‚  3.2 3B Inst â”‚                   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture des composants Frontend

```
frontend/src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ Meeting/
â”‚   â”‚   â”œâ”€â”€ Meeting.tsx              # Composant principal
â”‚   â”‚   â”œâ”€â”€ AudioRecorder.tsx        # Gestion enregistrement + WebSocket
â”‚   â”‚   â”œâ”€â”€ TranscriptionView.tsx     # Affichage/Ã©dition transcription
â”‚   â”‚   â”œâ”€â”€ SummaryGenerator.tsx     # GÃ©nÃ©ration compte rendu
â”‚   â”‚   â”œâ”€â”€ SummaryActions.tsx       # Actions (copier, exporter)
â”‚   â”‚   â”œâ”€â”€ LanguageSelector.tsx     # SÃ©lection langue
â”‚   â”‚   â””â”€â”€ RecordingStats.tsx       # Statistiques temps rÃ©el
â”‚   â””â”€â”€ Prompts/
â”‚       â”œâ”€â”€ Prompts.tsx              # Composant principal
â”‚       â”œâ”€â”€ PromptList.tsx           # Liste des prompts
â”‚       â””â”€â”€ PromptForm.tsx           # Formulaire CRUD
â”œâ”€â”€ contexts/
â”‚   â””â”€â”€ ThemeContext.tsx             # Gestion thÃ¨me dark/light
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api.ts                        # Client REST API
â”‚   â””â”€â”€ websocket.ts                  # Client WebSocket (non utilisÃ©)
â””â”€â”€ types/
    â””â”€â”€ index.ts                      # Types TypeScript
```

### Architecture des composants Backend

```
backend/app/
â”œâ”€â”€ main.py                           # Point d'entrÃ©e FastAPI
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ database.py                   # Configuration SQLAlchemy
â”‚   â””â”€â”€ seed.py                       # DonnÃ©es initiales
â”œâ”€â”€ models/
â”‚   â””â”€â”€ prompt.py                     # ModÃ¨le SQLAlchemy Prompt
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ prompts.py                    # Routes REST pour prompts
â”‚   â””â”€â”€ summary.py                    # Route gÃ©nÃ©ration compte rendu
â””â”€â”€ services/
    â”œâ”€â”€ whisper_service.py            # Service transcription Whisper
    â””â”€â”€ ollama_service.py            # Service gÃ©nÃ©ration LLM Ollama
```

---

## Stack technique

### Frontend

| Technologie | Version | Usage |
|------------|---------|-------|
| React | 18.2.0 | Framework UI |
| TypeScript | 5.2.2 | Typage statique |
| Vite | 7.3.1 | Build tool et dev server |
| React Router | 6.20.0 | Routing |
| jsPDF | 4.0.0 | Export PDF |

### Backend

| Technologie | Version | Usage |
|------------|---------|-------|
| Python | 3.10+ | Langage principal |
| FastAPI | 0.104.1 | Framework web async |
| Uvicorn | 0.24.0 | ASGI server |
| SQLAlchemy | 2.0.23 | ORM |
| SQLite | - | Base de donnÃ©es |
| Whisper | 20231117 | Transcription audio |
| OpenAI | 1.0.0+ | Client API OpenAI-compatible (pour Ollama) |
| Poetry | - | Gestion dÃ©pendances |

### Infrastructure

| Technologie | Usage |
|------------|-------|
| Docker | Containerisation |
| Docker Compose | Orchestration |
| Nginx | Reverse proxy (production) |
| ffmpeg | Conversion audio |
| Ollama | Serveur LLM local avec tÃ©lÃ©chargement automatique des modÃ¨les |

---

## Structure du projet

```
minuta-scribe/
â”œâ”€â”€ frontend/                         # Application React
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ logo.png                  # Logo de l'application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/               # Composants React
â”‚   â”‚   â”‚   â”œâ”€â”€ Meeting/              # Composants page Meeting
â”‚   â”‚   â”‚   â””â”€â”€ Prompts/              # Composants page Prompts
â”‚   â”‚   â”œâ”€â”€ contexts/                 # Contextes React
â”‚   â”‚   â”œâ”€â”€ services/                 # Services API
â”‚   â”‚   â”œâ”€â”€ types/                    # Types TypeScript
â”‚   â”‚   â”œâ”€â”€ App.tsx                   # Composant racine
â”‚   â”‚   â”œâ”€â”€ App.css                   # Styles globaux
â”‚   â”‚   â”œâ”€â”€ main.tsx                  # Point d'entrÃ©e
â”‚   â”‚   â””â”€â”€ index.css                 # Styles de base
â”‚   â”œâ”€â”€ package.json                  # DÃ©pendances npm
â”‚   â”œâ”€â”€ tsconfig.json                  # Config TypeScript
â”‚   â””â”€â”€ vite.config.ts                # Config Vite
â”‚
â”œâ”€â”€ backend/                          # API FastAPI
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                   # Point d'entrÃ©e FastAPI
â”‚   â”‚   â”œâ”€â”€ db/                       # Base de donnÃ©es
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py           # Config SQLAlchemy
â”‚   â”‚   â”‚   â””â”€â”€ seed.py               # DonnÃ©es initiales
â”‚   â”‚   â”œâ”€â”€ models/                   # ModÃ¨les SQLAlchemy
â”‚   â”‚   â”‚   â””â”€â”€ prompt.py
â”‚   â”‚   â”œâ”€â”€ routes/                   # Routes API
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts.py           # CRUD prompts
â”‚   â”‚   â”‚   â””â”€â”€ summary.py            # GÃ©nÃ©ration compte rendu
â”‚   â”‚   â””â”€â”€ services/                # Services mÃ©tier
â”‚   â”‚       â”œâ”€â”€ whisper_service.py    # Transcription
â”‚   â”‚       â””â”€â”€ ollama_service.py     # LLM
â”‚   â”œâ”€â”€ pyproject.toml                # Config Poetry
â”‚   â”œâ”€â”€ env.example                   # Exemple variables env
â”‚   â””â”€â”€ minuta.db                     # Base SQLite (gÃ©nÃ©rÃ©)
â”‚
â”œâ”€â”€ docker/                           # Configuration Docker
â”‚   â”œâ”€â”€ Dockerfile.backend            # Image backend
â”‚   â”œâ”€â”€ Dockerfile.frontend           # Image frontend
â”‚   â”œâ”€â”€ docker-compose.yml            # Orchestration
â”‚   â””â”€â”€ nginx.conf                    # Config Nginx
â”‚
â”œâ”€â”€ README.md                         # Documentation utilisateur
â”œâ”€â”€ README_TECH.md                    # Documentation technique (ce fichier)
â””â”€â”€ start.sh                          # Script dÃ©marrage rapide
```

---

## Flux de donnÃ©es

### Flux de transcription

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚
â”‚  (User)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 1. getUserMedia()
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MediaRecorder API  â”‚
â”‚  (audio/webm;opus) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 2. Chunks (100ms)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WebSocket Client   â”‚
â”‚  (AudioRecorder.tsx)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 3. ws.send(chunk)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI WebSocket  â”‚
â”‚  (/ws/transcribe)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 4. Accumulate chunks
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WhisperService     â”‚
â”‚  - Convert webmâ†’WAV â”‚
â”‚  - Transcribe       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 5. Text result
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WebSocket Response  â”‚
â”‚  {type: "partial"}  â”‚
â”‚  {type: "final"}    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 6. Update UI
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TranscriptionView  â”‚
â”‚  (Editable textarea)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de gÃ©nÃ©ration de compte rendu

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User clicks       â”‚
â”‚   "Generate"        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SummaryGenerator   â”‚
â”‚  - Select prompt    â”‚
â”‚  - Select model     â”‚
â”‚  - Get transcriptionâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 1. POST /api/generate-summary
       â”‚    {prompt_id, transcription, model}
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Route      â”‚
â”‚  (/api/generate-...)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 2. Get prompt from DB
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OllamaService      â”‚
â”‚  - Call Ollama API  â”‚
â”‚  - Model: mistral   â”‚
â”‚    or llama3.2      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 3. Return summary
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Display Summary    â”‚
â”‚  + Export options   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de gestion des prompts

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Prompts Page      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€ GET /api/prompts
       â”‚   â””â”€â–º List all prompts
       â”‚
       â”œâ”€â”€ POST /api/prompts
       â”‚   â””â”€â–º Create prompt
       â”‚
       â”œâ”€â”€ PUT /api/prompts/{id}
       â”‚   â””â”€â–º Update prompt
       â”‚
       â””â”€â”€ DELETE /api/prompts/{id}
           â””â”€â–º Delete prompt
```

---

## SchÃ©mas

### SchÃ©ma de base de donnÃ©es

```sql
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      prompts         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK) INTEGER     â”‚
â”‚ title VARCHAR       â”‚
â”‚ content TEXT        â”‚
â”‚ created_at DATETIME â”‚
â”‚ updated_at DATETIME â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ModÃ¨le SQLAlchemy :**
```python
class Prompt(Base):
    __tablename__ = "prompts"
    
    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, nullable=False)
    content = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
```

### SchÃ©ma de communication WebSocket

**Messages Client â†’ Server :**
```json
// Initialisation (langue)
{"language": "fr"}

// Chunk audio (bytes)
<binary data>

// ArrÃªt enregistrement
{"type": "stop"}
```

**Messages Server â†’ Client :**
```json
// Transcription partielle
{"type": "partial", "text": "Bonjour, comment allez-vous..."}

// Transcription finale
{"type": "final", "text": "Transcription complÃ¨te..."}

// Erreur
{"type": "error", "message": "Erreur lors de la transcription"}
```

### SchÃ©ma d'API REST

**GET /api/prompts**
```json
Response: [
  {
    "id": 1,
    "title": "Compte rendu standard",
    "content": "Tu es un assistant...",
    "created_at": "2024-01-01T00:00:00",
    "updated_at": "2024-01-01T00:00:00"
  }
]
```

**POST /api/generate-summary**
```json
Request: {
  "prompt_id": 1,
  "transcription": "Transcription text...",
  "model": "mistral:7b-instruct"
}

Response: {
  "summary": "Compte rendu gÃ©nÃ©rÃ©..."
}
```

**ModÃ¨les disponibles :**
- `mistral:7b-instruct` : Mistral 7B Instruct (par dÃ©faut, 4.4 GB)
- `llama3.2:3b` : Llama 3.2 3B Instruct (2.0 GB)

---

## Installation et dÃ©veloppement

### PrÃ©requis

- **Python 3.10+** avec pip
- **Node.js 18+** et npm
- **ffmpeg** (conversion audio)
- **Docker** (optionnel, pour dÃ©ploiement)

**Support multi-plateforme :**
- **macOS** : Terminal natif, Docker Desktop
- **Linux** : Terminal natif, Docker Engine ou Docker Desktop
- **Windows** : 
  - **Git Bash** (requis) : Inclus avec Git for Windows, permet d'exÃ©cuter les scripts bash. Si Git Bash n'est pas installÃ©, le script `start.sh` vous proposera de l'installer automatiquement
  - **Docker Desktop pour Windows** : Requis pour exÃ©cuter les conteneurs Docker

### Installation locale

```bash
# Cloner le repository
git clone <repository-url>
cd minuta-scribe

# Backend
cd backend
python3 -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
pip install -r requirements.txt
cp env.example .env
# Pour dÃ©veloppement local, configurer OLLAMA_BASE_URL=http://localhost:11434
# si Ollama tourne localement, sinon utiliser Docker Compose

# Frontend
cd ../frontend
npm install
```

### DÃ©veloppement

**Backend :**
```bash
cd backend
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
uvicorn app.main:app --reload --port 8000
```

**Frontend :**
```bash
cd frontend
npm run dev
```

**Tests :**
```bash
# Backend
cd backend
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
pytest  # Si pytest est installÃ©

# Frontend
cd frontend
npm run lint
```

### Scripts utiles

```bash
# Script de dÃ©marrage rapide (fonctionne sur Mac, Linux et Windows via Git Bash)
./start.sh

# Script de dÃ©sinstallation (fonctionne sur Mac, Linux et Windows via Git Bash)
./uninstall.sh

# Formatage code (si black est installÃ©)
cd backend
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
black app/  # Si black est installÃ©: pip install black
cd ../frontend && npm run lint -- --fix
```

**Note Windows :** Les scripts `start.sh` et `uninstall.sh` sont des scripts bash et nÃ©cessitent Git Bash pour Ãªtre exÃ©cutÃ©s sur Windows. Si Git Bash n'est pas installÃ©, le script `start.sh` vous proposera de l'installer automatiquement. Les scripts ne fonctionnent pas directement dans PowerShell ou l'Invite de commandes Windows.

---

## Configuration

### Variables d'environnement

**Backend (.env) :**
```env
# URL de Ollama (optionnel, valeur par dÃ©faut dans Docker: http://ollama:11434)
# OLLAMA_BASE_URL=http://ollama:11434
DATABASE_URL=sqlite:///./minuta.db
```

**Docker :**
Les variables d'environnement sont configurÃ©es automatiquement dans `docker-compose.yml`. Aucune configuration manuelle requise.

### Configuration Whisper

ModÃ¨le par dÃ©faut : `small` (bon compromis vitesse/qualitÃ©)

Options disponibles dans `backend/app/services/whisper_service.py` :
- `tiny` : Plus rapide, moins prÃ©cis
- `base` : Rapide, prÃ©cision moyenne
- `small` : **DÃ©faut** - Bon compromis
- `medium` : Plus lent, plus prÃ©cis
- `large` : TrÃ¨s lent, trÃ¨s prÃ©cis

### Configuration Ollama

ModÃ¨les disponibles :
- `mistral:7b-instruct` : Mistral 7B Instruct (par dÃ©faut, 4.4 GB)
- `llama3.2:3b` : Llama 3.2 3B Instruct (2.0 GB)

Les modÃ¨les sont automatiquement tÃ©lÃ©chargÃ©s au dÃ©marrage via le script `start.sh`. Si les modÃ¨les ne sont pas disponibles, ils seront tÃ©lÃ©chargÃ©s au premier usage. Pour tÃ©lÃ©charger manuellement, on peut exÃ©cuter `docker exec minuta-ollama ollama pull mistral:7b-instruct` et `docker exec minuta-ollama ollama pull llama3.2:3b`.

Configuration modifiable dans `backend/app/services/ollama_service.py`

---

## API Documentation

### Endpoints REST

#### Prompts

- `GET /api/prompts` - Liste tous les prompts
- `GET /api/prompts/{id}` - RÃ©cupÃ¨re un prompt
- `POST /api/prompts` - CrÃ©e un prompt
- `PUT /api/prompts/{id}` - Met Ã  jour un prompt
- `DELETE /api/prompts/{id}` - Supprime un prompt

#### Summary

- `POST /api/summary/generate` - GÃ©nÃ¨re un compte rendu

### WebSocket

- `WS /ws/transcribe` - Transcription en temps rÃ©el

**Documentation interactive :** http://localhost:8000/docs (Swagger UI)

---

## Tests

### Structure des tests

```
backend/
â””â”€â”€ tests/
    â”œâ”€â”€ test_prompts.py
    â”œâ”€â”€ test_summary.py
    â””â”€â”€ test_whisper_service.py
```

### ExÃ©cution

```bash
cd backend
poetry run pytest
poetry run pytest --cov=app tests/
```

---

## DÃ©ploiement

### Docker

**Build et lancement :**
```bash
cd docker
docker-compose up --build
```

> **Note :** Aucune configuration manuelle requise ! Les modÃ¨les LLM sont automatiquement tÃ©lÃ©chargÃ©s au dÃ©marrage via le script `start.sh`. Le premier tÃ©lÃ©chargement peut prendre plusieurs minutes (~6.4GB au total).

**Production :**
```bash
docker-compose -f docker-compose.prod.yml up -d
```

### Variables d'environnement production

- `OLLAMA_BASE_URL` : Optionnel (dÃ©faut: `http://ollama:11434`)
- `DATABASE_URL` : Optionnel (SQLite par dÃ©faut)
- `CORS_ORIGINS` : Origines autorisÃ©es

**PrÃ©requis systÃ¨me :**
- RAM : Au moins 8GB recommandÃ©s (16GB pour de meilleures performances)
- Espace disque : ~10-15GB pour les modÃ¨les LLM et les images Docker
- **Windows** : Git Bash (inclus avec Git for Windows) pour exÃ©cuter les scripts bash. Si Git Bash n'est pas installÃ©, le script `start.sh` vous proposera de l'installer automatiquement

---

## Contributions

### Guidelines

1. **Branches** : `feature/`, `fix/`, `docs/`
2. **Commits** : Messages clairs et descriptifs
3. **Code style** :
   - Python : Black (line-length: 100)
   - TypeScript : ESLint configurÃ©
4. **Tests** : Ajouter des tests pour nouvelles fonctionnalitÃ©s

### Workflow

1. Fork le repository
2. CrÃ©er une branche
3. Faire les modifications
4. Ajouter des tests
5. Soumettre une Pull Request

---

## Limitations et amÃ©liorations futures

### Limitations actuelles

1. **Audio systÃ¨me** : Seul le micro est supportÃ© (pas getDisplayMedia)
2. **Navigateurs** : Chrome/Edge recommandÃ©s
3. **Performance** : Transcription peut Ãªtre lente selon CPU
4. **Stockage** : Pas de persistance des transcriptions/comptes rendus (volontaire)
5. **Taille Docker** : 
   - Image backend ~2-3GB (Whisper)
   - Image Ollama ~2GB (base, modÃ¨les tÃ©lÃ©chargÃ©s sÃ©parÃ©ment)
   - ModÃ¨les LLM : ~6.4GB (Mistral 4.4GB + Llama 3.2 3B 2.0GB, tÃ©lÃ©chargÃ©s automatiquement au dÃ©marrage)
   - Total initial : ~4-5GB, puis ~10-15GB aprÃ¨s tÃ©lÃ©chargement des modÃ¨les

### AmÃ©liorations prÃ©vues

- [ ] Support audio systÃ¨me (getDisplayMedia)
- [ ] Historique des rÃ©unions
- [ ] Export Word/HTML
- [ ] Multi-langues (plus que FR/EN)
- [ ] Optimisation GPU (dÃ©jÃ  implÃ©mentÃ©, Ã  amÃ©liorer)
- [ ] Tests unitaires et d'intÃ©gration complets
- [ ] CI/CD pipeline

---

## Ressources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Whisper Documentation](https://github.com/openai/whisper)
- [Ollama Documentation](https://ollama.ai/)
- [Ollama API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [React Documentation](https://react.dev/)

---

## Licence

[Ã€ dÃ©finir]

---

**DerniÃ¨re mise Ã  jour :** Janvier 2026 (Version 2.0)
