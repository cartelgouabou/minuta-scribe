# Minuta - Documentation Technique

## ğŸ“‹ Table des matiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Architecture](#architecture)
3. [Stack technique](#stack-technique)
4. [Structure du projet](#structure-du-projet)
5. [Flux de donnÃ©es](#flux-de-donnÃ©es)
6. [SchÃ©mas](#schÃ©mas)
7. [Installation et dÃ©veloppement](#installation-et-dÃ©veloppement)
8. [Configuration](#configuration)
9. [API Documentation](#api-documentation)
10. [Tests](#tests)
11. [DÃ©ploiement](#dÃ©ploiement)
12. [Contributions](#contributions)

---

## Vue d'ensemble

**Minuta** est une application web **offline-first** pour la transcription en temps rÃ©el de rÃ©unions et la gÃ©nÃ©ration automatique de comptes rendus via un LLM. L'application permet Ã  l'utilisateur d'Ã©diter/annoter la transcription avant la gÃ©nÃ©ration du compte rendu.

### CaractÃ©ristiques principales

- **Offline-first** : Tout fonctionne localement par dÃ©faut. La transcription utilise Whisper local et la gÃ©nÃ©ration de compte rendu utilise Ollama avec le modÃ¨le LLM local (Llama 3.2 3B). Support optionnel de services cloud (Groq recommandÃ©, Vercel AI Gateway) pour des modÃ¨les plus performants (v2.2)
- **Temps rÃ©el** : Transcription partielle toutes les 3 secondes pendant l'enregistrement (v2.1)
- **Collage externe** : PossibilitÃ© de coller une transcription depuis une autre application (v2.1)
- **Multi-langues** : Support franÃ§ais et anglais
- **GPU automatique** : DÃ©tection et utilisation automatique du GPU si disponible (CUDA, MPS)
- **ThÃ¨me adaptatif** : Support dark/light mode avec toggle manuel
- **Ã‰dition du compte rendu** : PossibilitÃ© d'Ã©diter le compte rendu gÃ©nÃ©rÃ© avant export (v2.0)


## ğŸ“¦ Version 2.2 - 31 janvier 2026

### ğŸ‰ Nouvelles fonctionnalitÃ©s et amÃ©liorations

**Version 2.2** apporte le support des services cloud LLM et des amÃ©liorations significatives de l'expÃ©rience utilisateur.

#### âœ¨ FonctionnalitÃ©s ajoutÃ©es

1. **Support Groq et Vercel AI Gateway**
   - Service LLM unifiÃ© (`llm_service.py`) qui dÃ©tecte automatiquement le provider via variables d'environnement
   - Support de l'API Groq avec modÃ¨les optimisÃ©s : openai/gpt-oss-20b, llama-3.3-70b-versatile, qwen/qwen3-32b
   - Support de Vercel AI Gateway avec modÃ¨les : openai/gpt-oss-20b, alibaba/qwen-3-30b, google/gemini-2.0-flash-lite, meta/llama-4-scout
   - Endpoint API `/api/models` pour rÃ©cupÃ©rer dynamiquement les modÃ¨les disponibles selon le provider
   - Gestion d'erreurs amÃ©liorÃ©e avec messages explicites pour les problÃ¨mes d'API

2. **Configuration interactive amÃ©liorÃ©e**
   - Script `start.sh` avec configuration interactive des providers LLM
   - DÃ©tection et rÃ©utilisation automatique des clÃ©s API existantes
   - Nettoyage automatique des clÃ©s API (suppression des caractÃ¨res indÃ©sirables)
   - Configuration automatique de tous les modÃ¨les prÃ©dÃ©finis (plus besoin de sÃ©lection manuelle)

3. **Indicateur de transcription amÃ©liorÃ©**
   - Spinner visible pendant toute la durÃ©e de la transcription
   - Ã‰tat `isTranscribing` pour suivre la transcription mÃªme aprÃ¨s l'arrÃªt de l'enregistrement
   - Indicateur visuel Ã  cÃ´tÃ© du titre et message sous le textarea pendant le traitement

#### ğŸ”§ AmÃ©liorations techniques

- ModÃ¨le par dÃ©faut changÃ© de Mistral 7B Ã  Llama 3.2 3B Instruct uniquement (plus lÃ©ger, ~2GB au lieu de ~6.4GB)
- Service LLM unifiÃ© avec dÃ©tection automatique du provider
- Healthcheck Docker augmentÃ© Ã  3 minutes pour laisser le temps au modÃ¨le Whisper de se charger
- Gestion amÃ©liorÃ©e des variables d'environnement avec support des fichiers `.env`

#### ğŸ“ Changements dans le code

**Backend** :
- `llm_service.py` (nouveau) : Service unifiÃ© pour Ollama, Groq et Vercel
- `summary.py` : Utilise `LLMService`, endpoint `/api/models` ajoutÃ©
- `docker-compose.yml` : Support des variables Groq/Vercel via `env_file`

**Frontend** :
- `SummaryGenerator.tsx` : RÃ©cupÃ©ration dynamique des modÃ¨les via API
- `TranscriptionView.tsx` : Indicateur de transcription amÃ©liorÃ© avec spinner
- `AudioRecorder.tsx` : Gestion de l'Ã©tat `isTranscribing`
- `api.ts` : Fonction `getModels()` ajoutÃ©e
- `types/index.ts` : Type `ModelsResponse` ajoutÃ©

**Scripts** :
- `start.sh` : Configuration interactive des providers, gestion des clÃ©s API, nettoyage automatique

---

## ğŸ“¦ Version 2.1 - 26 janvier 2026

### ğŸ‰ Nouvelles fonctionnalitÃ©s et amÃ©liorations

**Version 2.1** apporte des amÃ©liorations significatives de performance et d'expÃ©rience utilisateur.

#### âœ¨ FonctionnalitÃ©s ajoutÃ©es

1. **Collage de transcription externe**
   - PossibilitÃ© de coller une transcription depuis une autre application directement dans le champ de transcription
   - Le gÃ©nÃ©rateur de compte rendu s'affiche automatiquement dÃ¨s qu'il y a du texte, mÃªme sans enregistrement
   - Modification de `TranscriptionView.tsx` pour permettre l'Ã©dition mÃªme sans enregistrement
   - Modification de `Meeting.tsx` pour afficher `SummaryGenerator` dÃ¨s qu'il y a une transcription

2. **Transcriptions partielles en temps rÃ©el**
   - Affichage progressif de la transcription pendant l'enregistrement (toutes les 3 secondes)
   - AmÃ©lioration significative de l'expÃ©rience utilisateur
   - Backend : ImplÃ©mentation avec `ThreadPoolExecutor` pour ne pas bloquer le WebSocket
   - Backend : Fonction `transcribe_partial()` asynchrone pour transcrire pÃ©riodiquement
   - Frontend : Gestion amÃ©liorÃ©e des messages `partial` avec fusion intelligente

3. **Optimisations de performance**
   - **PrÃ©chargement du modÃ¨le Whisper** : Le modÃ¨le est chargÃ© au dÃ©marrage de l'application pour Ã©viter les dÃ©lais lors de la premiÃ¨re transcription
   - **ParamÃ¨tres Whisper optimisÃ©s** : `best_of=1` (au lieu de 2) et `beam_size=3` (au lieu de 5) pour une transcription plus rapide
   - **Thread pool** : Utilisation de `ThreadPoolExecutor` pour les transcriptions afin de ne pas bloquer le WebSocket

#### ğŸ› Corrections

1. **Correction de la duplication du dernier mot**
   - Nouvelle fonction `mergeTranscription()` dans `AudioRecorder.tsx`
   - DÃ©tection intelligente des chevauchements de texte en comparant les derniers mots de la transcription accumulÃ©e avec les premiers mots du nouveau texte
   - Ã‰vite les rÃ©pÃ©titions et les doublons dans les transcriptions partielles

#### ğŸ“ Changements dans le code

**Frontend** :
- `TranscriptionView.tsx` : Permet l'Ã©dition mÃªme sans enregistrement, suppression de la variable `wasRecording` inutilisÃ©e
- `Meeting.tsx` : Affiche `SummaryGenerator` dÃ¨s qu'il y a une transcription (mÃªme collÃ©e manuellement)
- `AudioRecorder.tsx` : 
  - Nouvelle fonction `mergeTranscription()` pour dÃ©tecter et supprimer les chevauchements
  - AmÃ©lioration de la gestion des messages `partial` avec fusion intelligente

**Backend** :
- `main.py` : 
  - ImplÃ©mentation des transcriptions partielles avec `asyncio` et `ThreadPoolExecutor`
  - Fonction `transcribe_partial()` pour transcrire pÃ©riodiquement
  - PrÃ©chargement du modÃ¨le Whisper au dÃ©marrage
- `whisper_service.py` : 
  - MÃ©thode `preload_model()` pour prÃ©charger le modÃ¨le
  - Optimisation des paramÃ¨tres (`best_of=1`, `beam_size=3`)

---

## ğŸ“¦ Version 2.0 - Janvier 2026

### ğŸ‰ Nouvelles fonctionnalitÃ©s et amÃ©liorations

**Version 2.0** apporte des amÃ©liorations significatives pour une meilleure expÃ©rience utilisateur et une installation simplifiÃ©e.

#### âœ¨ FonctionnalitÃ©s ajoutÃ©es

1. **Ã‰dition du compte rendu**
   - Interface d'Ã©dition intÃ©grÃ©e avec `textarea` Ã©ditable
   - Indicateur visuel de modification (`isEdited` state)
   - Les exports PDF et TXT utilisent automatiquement le texte Ã©ditÃ©
   - Gestion d'Ã©tat sÃ©parÃ©e : `summary` (original) et `editedSummary` (modifiable)

2. **Support de plusieurs providers LLM**
   - Ollama (par dÃ©faut) : Llama 3.2 3B Instruct
   - Groq (recommandÃ©) : openai/gpt-oss-20b, llama-3.3-70b-versatile, qwen/qwen3-32b
   - Vercel AI Gateway : openai/gpt-oss-20b, alibaba/qwen-3-30b, google/gemini-2.0-flash-lite, meta/llama-4-scout
   - SÃ©lection du modÃ¨le via dropdown dans l'interface
   - TÃ©lÃ©chargement automatique des deux modÃ¨les au dÃ©marrage via `start.sh`
   - Validation cÃ´tÃ© backend des modÃ¨les disponibles

3. **Scripts d'installation et dÃ©sinstallation amÃ©liorÃ©s**
   - **`start.sh`** :
     - DÃ©tection automatique du systÃ¨me d'exploitation (macOS, Linux, Windows)
     - Installation automatique de Docker (macOS, Ubuntu/Debian)
     - Installation automatique de Git Bash sur Windows si nÃ©cessaire
     - TÃ©lÃ©chargement automatique du modÃ¨le LLM (Llama 3.2 3B) si Ollama est choisi
     - Configuration automatique des modÃ¨les si Groq ou Vercel est choisi
     - Messages d'aide spÃ©cifiques par plateforme
     - VÃ©rification de santÃ© des services Docker
   
   - **`uninstall.sh`** :
     - DÃ©sinstallation complÃ¨te de l'application
     - Suppression des conteneurs, images, volumes et rÃ©seaux Docker
     - Option de suppression de l'image Ollama
     - Confirmation avant suppression
     - DÃ©tection automatique du systÃ¨me d'exploitation

#### ğŸ”§ AmÃ©liorations techniques

- **Support multi-plateforme** :
  - DÃ©tection Windows via `OSTYPE` et `MSYSTEM`
  - Support Git Bash exclusif sur Windows (plus de WSL/PowerShell)
  - Messages d'erreur adaptÃ©s par plateforme

- **Configuration Nginx** :
  - Timeouts WebSocket augmentÃ©s (`proxy_read_timeout`, `proxy_send_timeout`)
  - Configuration optimisÃ©e pour les connexions longues

- **Gestion d'erreurs WebSocket** :
  - Timeout de 3 secondes avant affichage d'erreur
  - VÃ©rification de l'Ã©tat de connexion avant affichage
  - Gestion des fermetures normales vs erreurs

- **Variables CSS** :
  - Ajout de `--accent-color` et `--accent-rgb` pour cohÃ©rence visuelle
  - Styles pour textarea Ã©ditable avec focus states

#### ğŸ“ Changements dans le code

**Frontend** :
- `SummaryGenerator.tsx` : Ajout de l'Ã©tat `editedSummary` et `isEdited`
- `SummaryGenerator.tsx` : Remplacement de `div` par `textarea` Ã©ditable
- `SummaryActions.tsx` : Utilisation de `editedSummary` au lieu de `summary`
- `AudioRecorder.tsx` : AmÃ©lioration de la gestion d'erreurs WebSocket
- `Meeting.css` : Nouveaux styles pour textarea Ã©ditable et indicateur de modification
- `index.css` : Ajout des variables CSS d'accent

**Backend** :
- `routes/summary.py` : Validation des modÃ¨les LLM disponibles, endpoint `/api/models`
- `services/llm_service.py` : Service unifiÃ© pour Ollama, Groq et Vercel

**Infrastructure** :
- `docker/nginx.conf` : Configuration WebSocket amÃ©liorÃ©e
- `start.sh` : Logique d'installation multi-plateforme
- `uninstall.sh` : Script de dÃ©sinstallation complet

---

---

## Architecture

### Architecture gÃ©nÃ©rale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CLIENT (Browser)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              React Frontend (Vite)                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚   Meeting    â”‚  â”‚   Prompts    â”‚  â”‚  Theme      â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   Component  â”‚  â”‚   Component  â”‚  â”‚  Context    â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚         â”‚                  â”‚                         â”‚  â”‚
â”‚  â”‚         â”‚ WebSocket        â”‚ REST API                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                  â”‚
             â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERVER (FastAPI)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              FastAPI Application                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  WebSocket   â”‚  â”‚   REST API   â”‚  â”‚  Services  â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   Handler    â”‚  â”‚   Routes     â”‚  â”‚            â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚         â”‚                  â”‚              â”‚          â”‚  â”‚
â”‚  â”‚         â”‚                  â”‚              â”‚          â”‚  â”‚
â”‚  â”‚         â–¼                  â–¼              â–¼          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚   Whisper    â”‚  â”‚   Ollama     â”‚  â”‚  SQLite     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚   Service    â”‚  â”‚   Service    â”‚  â”‚  Database   â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ollama (Docker)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Ollama Server                            â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚  â”‚
â”‚  â”‚  â”‚    Llama     â”‚                   â”‚  â”‚
â”‚  â”‚  â”‚  3.2 3B Inst â”‚                   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture des composants Frontend

```
frontend/src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ Meeting/
â”‚   â”‚   â”œâ”€â”€ Meeting.tsx              # Composant principal
â”‚   â”‚   â”œâ”€â”€ AudioRecorder.tsx        # Gestion enregistrement + WebSocket
â”‚   â”‚   â”œâ”€â”€ TranscriptionView.tsx     # Affichage/Ã©dition transcription
â”‚   â”‚   â”œâ”€â”€ SummaryGenerator.tsx     # GÃ©nÃ©ration compte rendu
â”‚   â”‚   â”œâ”€â”€ SummaryActions.tsx       # Actions (copier, exporter)
â”‚   â”‚   â”œâ”€â”€ LanguageSelector.tsx     # SÃ©lection langue
â”‚   â”‚   â””â”€â”€ RecordingStats.tsx       # Statistiques temps rÃ©el
â”‚   â””â”€â”€ Prompts/
â”‚       â”œâ”€â”€ Prompts.tsx              # Composant principal
â”‚       â”œâ”€â”€ PromptList.tsx           # Liste des prompts
â”‚       â””â”€â”€ PromptForm.tsx           # Formulaire CRUD
â”œâ”€â”€ contexts/
â”‚   â””â”€â”€ ThemeContext.tsx             # Gestion thÃ¨me dark/light
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api.ts                        # Client REST API
â”‚   â””â”€â”€ websocket.ts                  # Client WebSocket (non utilisÃ©)
â””â”€â”€ types/
    â””â”€â”€ index.ts                      # Types TypeScript
```

### Architecture des composants Backend

```
backend/app/
â”œâ”€â”€ main.py                           # Point d'entrÃ©e FastAPI
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ database.py                   # Configuration SQLAlchemy
â”‚   â””â”€â”€ seed.py                       # DonnÃ©es initiales
â”œâ”€â”€ models/
â”‚   â””â”€â”€ prompt.py                     # ModÃ¨le SQLAlchemy Prompt
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ prompts.py                    # Routes REST pour prompts
â”‚   â””â”€â”€ summary.py                    # Route gÃ©nÃ©ration compte rendu
â””â”€â”€ services/
    â”œâ”€â”€ whisper_service.py            # Service transcription Whisper
    â””â”€â”€ ollama_service.py            # Service gÃ©nÃ©ration LLM Ollama
```

---

## Stack technique

### Frontend

| Technologie | Version | Usage |
|------------|---------|-------|
| React | 18.2.0 | Framework UI |
| TypeScript | 5.2.2 | Typage statique |
| Vite | 7.3.1 | Build tool et dev server |
| React Router | 6.20.0 | Routing |
| jsPDF | 4.0.0 | Export PDF |

### Backend

| Technologie | Version | Usage |
|------------|---------|-------|
| Python | 3.10+ | Langage principal |
| FastAPI | 0.104.1 | Framework web async |
| Uvicorn | 0.24.0 | ASGI server |
| SQLAlchemy | 2.0.23 | ORM |
| SQLite | - | Base de donnÃ©es |
| Whisper | 20231117 | Transcription audio |
| OpenAI | 1.0.0+ | Client API OpenAI-compatible (pour Ollama) |
| Poetry | - | Gestion dÃ©pendances |

### Infrastructure

| Technologie | Usage |
|------------|-------|
| Docker | Containerisation |
| Docker Compose | Orchestration |
| Nginx | Reverse proxy (production) |
| ffmpeg | Conversion audio |
| Ollama | Serveur LLM local avec tÃ©lÃ©chargement automatique des modÃ¨les |

---

## Structure du projet

```
minuta-scribe/
â”œâ”€â”€ frontend/                         # Application React
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ logo.png                  # Logo de l'application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/               # Composants React
â”‚   â”‚   â”‚   â”œâ”€â”€ Meeting/              # Composants page Meeting
â”‚   â”‚   â”‚   â””â”€â”€ Prompts/              # Composants page Prompts
â”‚   â”‚   â”œâ”€â”€ contexts/                 # Contextes React
â”‚   â”‚   â”œâ”€â”€ services/                 # Services API
â”‚   â”‚   â”œâ”€â”€ types/                    # Types TypeScript
â”‚   â”‚   â”œâ”€â”€ App.tsx                   # Composant racine
â”‚   â”‚   â”œâ”€â”€ App.css                   # Styles globaux
â”‚   â”‚   â”œâ”€â”€ main.tsx                  # Point d'entrÃ©e
â”‚   â”‚   â””â”€â”€ index.css                 # Styles de base
â”‚   â”œâ”€â”€ package.json                  # DÃ©pendances npm
â”‚   â”œâ”€â”€ tsconfig.json                  # Config TypeScript
â”‚   â””â”€â”€ vite.config.ts                # Config Vite
â”‚
â”œâ”€â”€ backend/                          # API FastAPI
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                   # Point d'entrÃ©e FastAPI
â”‚   â”‚   â”œâ”€â”€ db/                       # Base de donnÃ©es
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py           # Config SQLAlchemy
â”‚   â”‚   â”‚   â””â”€â”€ seed.py               # DonnÃ©es initiales
â”‚   â”‚   â”œâ”€â”€ models/                   # ModÃ¨les SQLAlchemy
â”‚   â”‚   â”‚   â””â”€â”€ prompt.py
â”‚   â”‚   â”œâ”€â”€ routes/                   # Routes API
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts.py           # CRUD prompts
â”‚   â”‚   â”‚   â””â”€â”€ summary.py            # GÃ©nÃ©ration compte rendu
â”‚   â”‚   â””â”€â”€ services/                # Services mÃ©tier
â”‚   â”‚       â”œâ”€â”€ whisper_service.py    # Transcription
â”‚   â”‚       â””â”€â”€ ollama_service.py     # LLM
â”‚   â”œâ”€â”€ pyproject.toml                # Config Poetry
â”‚   â”œâ”€â”€ env.example                   # Exemple variables env
â”‚   â””â”€â”€ minuta.db                     # Base SQLite (gÃ©nÃ©rÃ©)
â”‚
â”œâ”€â”€ docker/                           # Configuration Docker
â”‚   â”œâ”€â”€ Dockerfile.backend            # Image backend
â”‚   â”œâ”€â”€ Dockerfile.frontend           # Image frontend
â”‚   â”œâ”€â”€ docker-compose.yml            # Orchestration
â”‚   â””â”€â”€ nginx.conf                    # Config Nginx
â”‚
â”œâ”€â”€ README.md                         # Documentation utilisateur
â”œâ”€â”€ README_TECH.md                    # Documentation technique (ce fichier)
â””â”€â”€ start.sh                          # Script dÃ©marrage rapide
```

---

## Flux de donnÃ©es

### Flux de transcription

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚
â”‚  (User)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 1. getUserMedia()
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MediaRecorder API  â”‚
â”‚  (audio/webm;opus) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 2. Chunks (100ms)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WebSocket Client   â”‚
â”‚  (AudioRecorder.tsx)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 3. ws.send(chunk)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI WebSocket  â”‚
â”‚  (/ws/transcribe)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 4. Accumulate chunks
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WhisperService     â”‚
â”‚  - Convert webmâ†’WAV â”‚
â”‚  - Transcribe       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 5. Text result
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WebSocket Response  â”‚
â”‚  {type: "partial"}  â”‚
â”‚  {type: "final"}    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 6. Update UI
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TranscriptionView  â”‚
â”‚  (Editable textarea)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de gÃ©nÃ©ration de compte rendu

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User clicks       â”‚
â”‚   "Generate"        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SummaryGenerator   â”‚
â”‚  - Select prompt    â”‚
â”‚  - Select model     â”‚
â”‚  - Get transcriptionâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 1. POST /api/generate-summary
       â”‚    {prompt_id, transcription, model}
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Route      â”‚
â”‚  (/api/generate-...)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 2. Get prompt from DB
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OllamaService      â”‚
â”‚  - Call Ollama API  â”‚
â”‚  - Model: mistral   â”‚
â”‚    or llama3.2      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 3. Return summary
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Display Summary    â”‚
â”‚  + Export options   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flux de gestion des prompts

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Prompts Page      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€ GET /api/prompts
       â”‚   â””â”€â–º List all prompts
       â”‚
       â”œâ”€â”€ POST /api/prompts
       â”‚   â””â”€â–º Create prompt
       â”‚
       â”œâ”€â”€ PUT /api/prompts/{id}
       â”‚   â””â”€â–º Update prompt
       â”‚
       â””â”€â”€ DELETE /api/prompts/{id}
           â””â”€â–º Delete prompt
```

---

## SchÃ©mas

### SchÃ©ma de base de donnÃ©es

```sql
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      prompts         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK) INTEGER     â”‚
â”‚ title VARCHAR       â”‚
â”‚ content TEXT        â”‚
â”‚ created_at DATETIME â”‚
â”‚ updated_at DATETIME â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ModÃ¨le SQLAlchemy :**
```python
class Prompt(Base):
    __tablename__ = "prompts"
    
    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, nullable=False)
    content = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
```

### SchÃ©ma de communication WebSocket

**Messages Client â†’ Server :**
```json
// Initialisation (langue)
{"language": "fr"}

// Chunk audio (bytes)
<binary data>

// ArrÃªt enregistrement
{"type": "stop"}
```

**Messages Server â†’ Client :**
```json
// Transcription partielle
{"type": "partial", "text": "Bonjour, comment allez-vous..."}

// Transcription finale
{"type": "final", "text": "Transcription complÃ¨te..."}

// Erreur
{"type": "error", "message": "Erreur lors de la transcription"}
```

### SchÃ©ma d'API REST

**GET /api/prompts**
```json
Response: [
  {
    "id": 1,
    "title": "Compte rendu standard",
    "content": "Tu es un assistant...",
    "created_at": "2024-01-01T00:00:00",
    "updated_at": "2024-01-01T00:00:00"
  }
]
```

**POST /api/generate-summary**
```json
Request: {
  "prompt_id": 1,
  "transcription": "Transcription text...",
  "model": "mistral:7b-instruct"
}

Response: {
  "summary": "Compte rendu gÃ©nÃ©rÃ©..."
}
```

**ModÃ¨les disponibles :**
- `llama3.2:3b` : Llama 3.2 3B Instruct (par dÃ©faut, 2.0 GB)
- `llama3.2:3b` : Llama 3.2 3B Instruct (2.0 GB)

---

## Installation et dÃ©veloppement

### PrÃ©requis

- **Python 3.10+** avec pip
- **Node.js 18+** et npm
- **ffmpeg** (conversion audio)
- **Docker** (optionnel, pour dÃ©ploiement)

**Support multi-plateforme :**
- **macOS** : Terminal natif, Docker Desktop
- **Linux** : Terminal natif, Docker Engine ou Docker Desktop
- **Windows** : 
  - **Git Bash** (requis) : Inclus avec Git for Windows, permet d'exÃ©cuter les scripts bash. Si Git Bash n'est pas installÃ©, le script `start.sh` vous proposera de l'installer automatiquement
  - **Docker Desktop pour Windows** : Requis pour exÃ©cuter les conteneurs Docker

### Installation locale

```bash
# Cloner le repository
git clone <repository-url>
cd minuta-scribe

# Backend
cd backend
python3 -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
pip install -r requirements.txt
cp env.example .env
# Pour dÃ©veloppement local, configurer OLLAMA_BASE_URL=http://localhost:11434
# si Ollama tourne localement, sinon utiliser Docker Compose

# Frontend
cd ../frontend
npm install
```

### DÃ©veloppement

**Backend :**
```bash
cd backend
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
uvicorn app.main:app --reload --port 8000
```

**Frontend :**
```bash
cd frontend
npm run dev
```

**Tests :**
```bash
# Backend
cd backend
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
pytest  # Si pytest est installÃ©

# Frontend
cd frontend
npm run lint
```

### Scripts utiles

```bash
# Script de dÃ©marrage rapide (fonctionne sur Mac, Linux et Windows via Git Bash)
./start.sh

# Script de dÃ©sinstallation (fonctionne sur Mac, Linux et Windows via Git Bash)
./uninstall.sh

# Formatage code (si black est installÃ©)
cd backend
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
black app/  # Si black est installÃ©: pip install black
cd ../frontend && npm run lint -- --fix
```

**Note Windows :** Les scripts `start.sh` et `uninstall.sh` sont des scripts bash et nÃ©cessitent Git Bash pour Ãªtre exÃ©cutÃ©s sur Windows. Si Git Bash n'est pas installÃ©, le script `start.sh` vous proposera de l'installer automatiquement. Les scripts ne fonctionnent pas directement dans PowerShell ou l'Invite de commandes Windows.

---

## Configuration

### Variables d'environnement

**Backend (.env) :**
```env
# Configuration Ollama (par dÃ©faut, utilisÃ© si aucune clÃ© API cloud n'est configurÃ©e)
# URL de Ollama (optionnel, valeur par dÃ©faut dans Docker: http://ollama:11434)
OLLAMA_BASE_URL=http://ollama:11434

# Configuration Groq (optionnel, pour utiliser Groq au lieu d'Ollama)
GROQ_API_KEY=votre_cle_api_groq
LLM_MODELS=openai/gpt-oss-20b,llama-3.3-70b-versatile,qwen/qwen3-32b

# Configuration Vercel AI Gateway (optionnel, pour utiliser Vercel au lieu d'Ollama)
# AI_GATEWAY_API_KEY=votre_cle_api_vercel
# LLM_MODELS=openai/gpt-oss-20b,alibaba/qwen-3-30b,google/gemini-2.0-flash-lite,meta/llama-4-scout
# OLLAMA_BASE_URL=http://ollama:11434
DATABASE_URL=sqlite:///./minuta.db
```

**Docker :**
Les variables d'environnement sont configurÃ©es automatiquement dans `docker-compose.yml`. 

**Configuration via `start.sh` (recommandÃ©) :**
- Le script `start.sh` vous guide pour configurer Groq ou Vercel si vous le souhaitez
- Les clÃ©s API et modÃ¨les sont automatiquement enregistrÃ©s dans `backend/.env`
- Le fichier `.env` est exclu de Git pour protÃ©ger vos clÃ©s API

**Configuration manuelle :**
- CrÃ©ez `backend/.env` avec les variables appropriÃ©es selon le provider choisi
- Pour Groq : `GROQ_API_KEY=...` et `LLM_MODELS=...`
- Pour Vercel : `AI_GATEWAY_API_KEY=...` et `LLM_MODELS=...`
- Si aucune clÃ© API n'est configurÃ©e, Ollama sera utilisÃ© par dÃ©faut

### Configuration Whisper

ModÃ¨le par dÃ©faut : `small` (bon compromis vitesse/qualitÃ©)

Options disponibles dans `backend/app/services/whisper_service.py` :
- `tiny` : Plus rapide, moins prÃ©cis
- `base` : Rapide, prÃ©cision moyenne
- `small` : **DÃ©faut** - Bon compromis
- `medium` : Plus lent, plus prÃ©cis
- `large` : TrÃ¨s lent, trÃ¨s prÃ©cis

### Configuration Ollama

ModÃ¨les disponibles :
- `llama3.2:3b` : Llama 3.2 3B Instruct (par dÃ©faut, 2.0 GB)
- `llama3.2:3b` : Llama 3.2 3B Instruct (2.0 GB)

Le modÃ¨le Llama 3.2 3B est automatiquement tÃ©lÃ©chargÃ© au dÃ©marrage via le script `start.sh` si Ollama est choisi. Si le modÃ¨le n'est pas disponible, il sera tÃ©lÃ©chargÃ© au premier usage. Pour tÃ©lÃ©charger manuellement, on peut exÃ©cuter `docker exec minuta-ollama ollama pull llama3.2:3b`.

**Note :** Si vous utilisez Groq ou Vercel, aucun tÃ©lÃ©chargement de modÃ¨le local n'est nÃ©cessaire.

Configuration modifiable dans `backend/app/services/ollama_service.py`

---

## API Documentation

### Endpoints REST

#### Prompts

- `GET /api/prompts` - Liste tous les prompts
- `GET /api/prompts/{id}` - RÃ©cupÃ¨re un prompt
- `POST /api/prompts` - CrÃ©e un prompt
- `PUT /api/prompts/{id}` - Met Ã  jour un prompt
- `DELETE /api/prompts/{id}` - Supprime un prompt

#### Summary

- `POST /api/summary/generate` - GÃ©nÃ¨re un compte rendu

### WebSocket

- `WS /ws/transcribe` - Transcription en temps rÃ©el

**Documentation interactive :** http://localhost:8000/docs (Swagger UI)

---

## Tests

### Structure des tests

```
backend/
â””â”€â”€ tests/
    â”œâ”€â”€ test_prompts.py
    â”œâ”€â”€ test_summary.py
    â””â”€â”€ test_whisper_service.py
```

### ExÃ©cution

```bash
cd backend
poetry run pytest
poetry run pytest --cov=app tests/
```

---

## DÃ©ploiement

### Docker

**Build et lancement :**
```bash
cd docker
docker-compose up --build
```

> **Note :** Aucune configuration manuelle requise ! Le modÃ¨le LLM (Llama 3.2 3B) est automatiquement tÃ©lÃ©chargÃ© au dÃ©marrage via le script `start.sh` si Ollama est choisi. Le premier tÃ©lÃ©chargement peut prendre quelques minutes (~2.0GB). Avec Groq ou Vercel, aucun tÃ©lÃ©chargement n'est nÃ©cessaire.

**Production :**
```bash
docker-compose -f docker-compose.prod.yml up -d
```

### Variables d'environnement production

- `OLLAMA_BASE_URL` : Optionnel (dÃ©faut: `http://ollama:11434`)
- `DATABASE_URL` : Optionnel (SQLite par dÃ©faut)
- `CORS_ORIGINS` : Origines autorisÃ©es

**PrÃ©requis systÃ¨me :**
- RAM : Au moins 8GB recommandÃ©s (16GB pour de meilleures performances)
- Espace disque : ~10-15GB pour les modÃ¨les LLM et les images Docker
- **Windows** : Git Bash (inclus avec Git for Windows) pour exÃ©cuter les scripts bash. Si Git Bash n'est pas installÃ©, le script `start.sh` vous proposera de l'installer automatiquement

---

## Contributions

### Guidelines

1. **Branches** : `feature/`, `fix/`, `docs/`
2. **Commits** : Messages clairs et descriptifs
3. **Code style** :
   - Python : Black (line-length: 100)
   - TypeScript : ESLint configurÃ©
4. **Tests** : Ajouter des tests pour nouvelles fonctionnalitÃ©s

### Workflow

1. Fork le repository
2. CrÃ©er une branche
3. Faire les modifications
4. Ajouter des tests
5. Soumettre une Pull Request

---

## Limitations et amÃ©liorations futures

### Limitations actuelles

1. **Audio systÃ¨me** : Seul le micro est supportÃ© (pas getDisplayMedia)
2. **Navigateurs** : Chrome/Edge recommandÃ©s
3. **Performance** : Transcription peut Ãªtre lente selon CPU
4. **Stockage** : Pas de persistance des transcriptions/comptes rendus (volontaire)
5. **Taille Docker** : 
   - Image backend ~2-3GB (Whisper)
   - Image Ollama ~2GB (base, modÃ¨les tÃ©lÃ©chargÃ©s sÃ©parÃ©ment)
   - ModÃ¨les LLM (Ollama) : ~2.0GB (Llama 3.2 3B, tÃ©lÃ©chargÃ© automatiquement au dÃ©marrage)
   - Avec Groq ou Vercel : Aucun tÃ©lÃ©chargement de modÃ¨le local nÃ©cessaire
   - Total initial : ~4-5GB, puis ~10-15GB aprÃ¨s tÃ©lÃ©chargement des modÃ¨les

### AmÃ©liorations prÃ©vues

- [ ] Support audio systÃ¨me (getDisplayMedia)
- [ ] Historique des rÃ©unions
- [ ] Export Word/HTML
- [ ] Multi-langues (plus que FR/EN)
- [ ] Optimisation GPU (dÃ©jÃ  implÃ©mentÃ©, Ã  amÃ©liorer)
- [ ] Tests unitaires et d'intÃ©gration complets
- [ ] CI/CD pipeline

---

## Ressources

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Whisper Documentation](https://github.com/openai/whisper)
- [Ollama Documentation](https://ollama.ai/)
- [Ollama API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [React Documentation](https://react.dev/)

---

## Licence

[Ã€ dÃ©finir]

---

**DerniÃ¨re mise Ã  jour :** 26 janvier 2026 (Version 2.1)
